# åˆ†å¸ƒå¼çˆ¬è™«ç³»ç»Ÿ

ä¸€ä¸ªåŸºäºPythonçš„åˆ†å¸ƒå¼çˆ¬è™«ç³»ç»Ÿï¼Œæ”¯æŒå¤šèŠ‚ç‚¹å¹¶è¡Œçˆ¬å–ã€ä»»åŠ¡è°ƒåº¦ã€ç»“æœå­˜å‚¨ç­‰åŠŸèƒ½ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **hhhhhhåˆ†å¸ƒå¼æ¶æ„**: åŸºäºCeleryå’ŒRedisçš„åˆ†å¸ƒå¼ä»»åŠ¡å¤„ç†
- ğŸ”„ **å¤šç§çˆ¬å–æ¨¡å¼**: æ”¯æŒå•ä¸ªã€æ‰¹é‡ã€å¹¶è¡Œã€å¼‚æ­¥çˆ¬å–
- ğŸ¯ **çµæ´»é…ç½®**: æ”¯æŒä»£ç†ã€ç”¨æˆ·ä»£ç†è½®æ¢ã€è¯·æ±‚å»¶è¿Ÿç­‰é…ç½®
- ğŸ“Š **ä»»åŠ¡ç›‘æ§**: å®æ—¶ç›‘æ§ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€å’Œç»Ÿè®¡ä¿¡æ¯
- ğŸŒ **Web API**: æä¾›RESTful APIæ¥å£ç®¡ç†çˆ¬è™«ä»»åŠ¡
- ğŸ’¾ **æ•°æ®å­˜å‚¨**: æ”¯æŒRedisç¼“å­˜å’Œå¤šç§æ•°æ®åº“å­˜å‚¨
- ğŸ”§ **æ˜“äºæ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ·»åŠ æ–°çš„çˆ¬è™«åŠŸèƒ½

## ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ä¸»èŠ‚ç‚¹        â”‚    â”‚   å·¥ä½œèŠ‚ç‚¹1     â”‚    â”‚   å·¥ä½œèŠ‚ç‚¹2     â”‚
â”‚  (è°ƒåº¦å™¨)       â”‚    â”‚  (çˆ¬è™«è¿›ç¨‹)     â”‚    â”‚  (çˆ¬è™«è¿›ç¨‹)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Redisé˜Ÿåˆ—     â”‚
                    â”‚  (ä»»åŠ¡åˆ†å‘)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## å®‰è£…ä¾èµ–

```bash
# å®‰è£…Pythonä¾èµ–
pip install -r requirements.txt

# å®‰è£…Redis (Ubuntu/Debian)
sudo apt-get install redis-server

# å®‰è£…Redis (macOS)
brew install redis

# å¯åŠ¨RedisæœåŠ¡
redis-server
```

## å¿«é€Ÿå¼€å§‹

### 1. é…ç½®ç¯å¢ƒ

å¤åˆ¶å¹¶ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼š

```bash
cp .env.example .env
```

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œé…ç½®Redisè¿æ¥ç­‰ä¿¡æ¯ï¼š

```env
# Redisé…ç½®
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# çˆ¬è™«é…ç½®
CRAWLER_DELAY=1.0
CRAWLER_TIMEOUT=30
MAX_RETRIES=3
MAX_WORKERS=4
```

### 2. å¯åŠ¨å·¥ä½œèŠ‚ç‚¹

```bash
# å¯åŠ¨å·¥ä½œèŠ‚ç‚¹
python worker.py --mode worker

# å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
python worker.py --mode beat

# å¯åŠ¨ä»»åŠ¡ç›‘æ§
python worker.py --mode monitor
```

### 3. å¯åŠ¨Web API

```bash
# å¯åŠ¨APIæœåŠ¡
python api/app.py
```

### 4. ä½¿ç”¨ç¤ºä¾‹

```python
from scheduler import CrawlerScheduler

# åˆ›å»ºè°ƒåº¦å™¨
scheduler = CrawlerScheduler()

# å•ä¸ªURLçˆ¬å–
task_id = scheduler.add_url_task("https://example.com", parser_type='html')

# æ‰¹é‡URLçˆ¬å–
urls = ["https://example1.com", "https://example2.com"]
task_id = scheduler.add_batch_task(urls, parser_type='html')

# å¹¶è¡Œçˆ¬å–
result = scheduler.execute_parallel_tasks(urls, parser_type='html')
```

## APIæ¥å£

### åˆ›å»ºä»»åŠ¡

```bash
# å•ä¸ªä»»åŠ¡
curl -X POST http://localhost:5000/api/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "type": "single",
    "url": "https://example.com",
    "parser_type": "html"
  }'

# æ‰¹é‡ä»»åŠ¡
curl -X POST http://localhost:5000/api/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "type": "batch",
    "urls": ["https://example1.com", "https://example2.com"],
    "parser_type": "html"
  }'
```

### æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

```bash
curl http://localhost:5000/api/tasks/{task_id}
```

### è·å–ä»»åŠ¡ç»“æœ

```bash
curl http://localhost:5000/api/tasks/{task_id}/result
```

### è·å–ç»Ÿè®¡ä¿¡æ¯

```bash
curl http://localhost:5000/api/tasks
```

## é…ç½®è¯´æ˜

### çˆ¬è™«é…ç½®

- `CRAWLER_DELAY`: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
- `CRAWLER_TIMEOUT`: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
- `MAX_RETRIES`: æœ€å¤§é‡è¯•æ¬¡æ•°
- `MAX_WORKERS`: æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
- `MAX_CONCURRENT_REQUESTS`: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°

### ä»£ç†é…ç½®

```env
USE_PROXY=true
PROXY_LIST=http://proxy1:8080,http://proxy2:8080
```

### ç”¨æˆ·ä»£ç†é…ç½®

```env
USER_AGENT_ROTATION=true
```

## è‡ªå®šä¹‰æ•°æ®æå–

æ”¯æŒé€šè¿‡CSSé€‰æ‹©å™¨è‡ªå®šä¹‰æ•°æ®æå–è§„åˆ™ï¼š

```python
extract_rules = {
    'title': {
        'type': 'text',
        'selector': 'h1.title'
    },
    'content': {
        'type': 'text',
        'selector': 'div.content'
    },
    'links': {
        'type': 'list',
        'selector': 'a.link'
    },
    'image': {
        'type': 'attr',
        'selector': 'img.cover',
        'attr': 'src'
    }
}
```

## éƒ¨ç½²è¯´æ˜

### Dockeréƒ¨ç½²

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "api/app.py"]
```

### ç”Ÿäº§ç¯å¢ƒé…ç½®

1. ä½¿ç”¨ç”Ÿäº§çº§Redisé›†ç¾¤
2. é…ç½®è´Ÿè½½å‡è¡¡å™¨
3. è®¾ç½®æ—¥å¿—è½®è½¬
4. é…ç½®ç›‘æ§å‘Šè­¦
5. ä½¿ç”¨HTTPS

## ç›‘æ§å’Œæ—¥å¿—

### æ—¥å¿—é…ç½®

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log'),
        logging.StreamHandler()
    ]
)
```

### æ€§èƒ½ç›‘æ§

- ä»»åŠ¡æ‰§è¡Œæ—¶é—´
- æˆåŠŸç‡ç»Ÿè®¡
- é˜Ÿåˆ—é•¿åº¦ç›‘æ§
- èµ„æºä½¿ç”¨æƒ…å†µ

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **Redisè¿æ¥å¤±è´¥**
   - æ£€æŸ¥RedisæœåŠ¡æ˜¯å¦å¯åŠ¨
   - éªŒè¯è¿æ¥é…ç½®

2. **ä»»åŠ¡æ‰§è¡Œå¤±è´¥**
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - éªŒè¯ç›®æ ‡ç½‘ç«™å¯è®¿é—®æ€§
   - æŸ¥çœ‹é”™è¯¯æ—¥å¿—

3. **æ€§èƒ½é—®é¢˜**
   - è°ƒæ•´å¹¶å‘æ•°é…ç½®
   - ä¼˜åŒ–è¯·æ±‚é—´éš”
   - ä½¿ç”¨ä»£ç†æ± 

## è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. æ¨é€åˆ°åˆ†æ”¯
5. åˆ›å»º Pull Request

## è®¸å¯è¯

MIT License

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ–è”ç³»å¼€å‘è€…ã€‚ 